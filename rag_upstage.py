# -*- coding: utf-8 -*-
"""RAG_upstage.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1grrJidtHi1EX43C3pb97hJ6GKIn6LMmm

# 1. Load Environment Variables
"""

import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain.retrievers import EnsembleRetriever
from langchain.prompts import ChatPromptTemplate

load_dotenv()

"""# 2. Model (Upstage)"""

upstage_api_key = os.getenv("UPSTAGE_API_KEY")
base_url = "https://api.upstage.ai/v1/solar"
tavily_api_key = os.getenv("TAVILY_API_KEY")

client = OpenAI(
    api_key=upstage_api_key,
    base_url=base_url
)

stream = client.chat.completions.create(
    model="solar-pro2",
    messages=[
        {
            "role": "user",
            "content": "Hi, how are you?"
        }
    ],
    stream=True,
)

for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="")

# Use with stream=False
# print(stream.choices[0].message.content)

"""# 3. Set Up Vector Store"""

import trafilatura #웹 페이지에서 본문만 자동 추출하는 Python 패키지-<script>, <nav>, 광고 등 제거
from langchain.schema import Document
data_dir="C:/Users/zoode/Desktop/capstone/crawling/data"
docs = []

for file in os.listdir(data_dir):
    if file.endswith(".html"):
        file_path = os.path.join(data_dir, file)
        with open(file_path, "r", encoding="utf-8") as f:
            html_content = f.read()
        text = trafilatura.extract(html_content)  # 핵심 본문만 자동 추출
        if text:
            docs.append(Document(page_content=text))

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
docs = splitter.split_documents(docs)

embedding_model = UpstageEmbeddings(upstage_api_key=upstage_api_key, model="embedding-query")

db = Chroma.from_documents(
    documents=docs,
    embedding = embedding_model,
    persist_directory="./chroma_db",
    collection_name = "Creation_evolution"
)

retriever = db.as_retriever(
    search_type="similarity",  # threshold 없이 단순 유사도 검색
    search_kwargs={"k": 3}
)

question="자연선택은 효율적인가, 비효율적인가?"

results=retriever.invoke(question) #입력한 질문과 의미적으로 가장 가까운 문서를 찾아서 반환

results

context=results[0].page_content

print(context)

"""**Augment**"""

from langchain.prompts import PromptTemplate
from langchain.prompts import HumanMessagePromptTemplate
from langchain_core.messages import SystemMessage
from langchain.prompts import ChatPromptTemplate

chat_template=ChatPromptTemplate.from_messages(
    [
        SystemMessage(
            content="당신은 창조과학을 연구하는 교수입니다"
        ),
        HumanMessagePromptTemplate.from_template(
            """
            {question}
            아래의 문맥에 기반하여 답해주세요.
            {context}
            """
        )
    ]
)

message=chat_template.format_messages(
    question=question,
    context=context
)

model = ChatOpenAI(
    api_key=upstage_api_key,
    base_url=base_url,
    model="solar-pro2",
)

for chunk in model.stream(message):
  print(chunk.content, end="", flush=True)